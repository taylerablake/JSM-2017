\documentclass[12pt]{article}
\usepackage{graphicx,psfrag,amsfonts,float,mathbbol,xcolor,cleveref}
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage[mathscr]{euscript}
\usepackage{enumitem}
\usepackage{accents}
\usepackage{framed}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{mathtools}
\usepackage{IEEEtrantools}
\usepackage{times}
\usepackage{cite}
\usepackage{amsthm}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{tabularx,ragged2e,booktabs,caption}
\newcolumntype{C}[1]{>{\Centering}m{#1}}
\renewcommand\tabularxcolumn[1]{C{#1}}
\usepackage[letterpaper, left=1in, top=1in, right=1in, bottom=1in,nohead,includefoot, verbose, ignoremp]{geometry}
\newcommand{\comment}[1]{\text{\phantom{(#1)}} \tag{#1}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand*\needsparaphrased{\color{red}}
\newcommand*\needscited{\color{orange}}
\newcommand*\needsproof{\color{blue}}
\newcommand*\outlineskeleton{\color{green}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\bfeps}{\mbox{\boldmath $\epsilon$}}
\newcommand{\bfgamma}{\mbox{\boldmath $\gamma$}}
\newcommand{\bflam}{\mbox{\boldmath $\lambda$}}
\newcommand{\bfphi}{\mbox{\boldmath $\phi$}}
\newcommand{\bfsigma}{\mbox{\boldmath $\sigma$}}
\newcommand{\bfbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\bfalpha}{\mbox{\boldmath $\alpha$}}
\newcommand{\bfe}{\mbox{\boldmath $e$}}
\newcommand{\bff}{\mbox{\boldmath $f$}}
\newcommand{\bfone}{\mbox{\boldmath $1$}}
\newcommand{\bft}{\mbox{\boldmath $t$}}
\newcommand{\bfo}{\mbox{\boldmath $0$}}
\newcommand{\bfO}{\mbox{\boldmath $O$}}
\newcommand{\bfx}{\mbox{\boldmath $x$}}
\newcommand{\bfX}{\mbox{\boldmath $X$}}
\newcommand{\bfz}{\mbox{\boldmath $z$}}


\newcommand{\bfm}{\mbox{\boldmath $m}}
\newcommand{\bfy}{\mbox{\boldmath $y$}}
\newcommand{\bfa}{\mbox{\boldmath $a$}}
\newcommand{\bfb}{\mbox{\boldmath $b$}}
\newcommand{\bfY}{\mbox{\boldmath $Y$}}
\newcommand{\bfS}{\mbox{\boldmath $S$}}
\newcommand{\bfZ}{\mbox{\boldmath $Z$}}
\newcommand{\cardT}{\vert \mathcal{T} \vert}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\def\bL{\mathbf{L}}


\makeatletter
\renewcommand{\theenumi}{\Roman{enumi}}
\renewcommand{\labelenumi}{\theenumi.}
\renewcommand{\theenumii}{\Alph{enumii}}
\renewcommand{\labelenumii}{\theenumii.}
\renewcommand{\p@enumii}{\theenumi.}
\makeatother

\begin{document}

%\nocite{*}
\def\bL{\mathbf{L}}
%\usepackage{mathtime}

%%UNCOMMENT following line if you have package


\title{ Nonparametric Covariance Estimation for Longitudinal Data via Penalized Tensor Product Splines}

\author{Tayler A. Blake\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201} \and  Yoonkyung Lee\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201}}

\bibliographystyle{plainnat}
\maketitle

\begin{abstract}
With high dimensional longitudinal and functional data becoming much more common, there is a strong need for methods of estimating large covariance matrices. Estimation is made difficult  by the instability of sample covariance matrices in high dimensions and a positive-definite constraint we desire to impose on estimates. A Cholesky decomposition of the covariance matrix allows for parameter estimation via unconstrained optimization as well as a statistically meaningful interpretation of the parameter estimates. Regularization improves stability of covariance estimates in high dimensions, as well as in the case where functional data are sparse and individual curves are sampled at different and possibly unequally spaced time points. By viewing the entries of the covariance matrix as the evaluation of a continuous bivariate function at the pairs of observed time points, we treat covariance estimation as bivariate smoothing. 

Within regularization framework, we propose novel covariance penalties which are designed to yield natural null models presented in the literature for stationarity or short-term dependence. These penalties are expressed in terms of variation in continuous time lag and its orthogonal complement. We present numerical results and data analysis to illustrate the utility of the proposed method. \\
\\
%\begin{keywords}
{\bf keywords:} non-parametric, covariance, longitudinal data, functional data, splines, reproducing kernel Hilbert space
%\end{keywords}
\end{abstract}

\section{Introduction}

An estimate of the covariance matrix or its inverse is required for nearly all statistical procedures in classical multivariate data analysis, time series analysis, spatial statistics and, more recently, the growing field of statistical learning. Covariance estimates play a critical role in the performance of techniques for clustering and classification such as linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), factor analysis, and principal components analysis (PCA), analysis of conditional independence through graphical models, classical multivariate regression, prediction, and Kriging. Covariance estimation with high dimensional data has has recently gained growing interest; it is generally recognized that there are two primary hurdles responsible for the difficulty in covariance estimation: the instability of sample covariance matrices in high dimensions and a positive-definite constraint we wish estimates to obey.

Prevalent technological advances in industry and many areas of science make high dimensional longitudinal and functional data a common occurrence, arising in numerous areas including medicine, public health, biology, and environmental science with specific applications including fMRI, spectroscopic imaging, gene microarrays among many others, presenting a need for effective covariance estimation in the challenging situation where parameter dimensionality is possibly much larger than the number of observations. Additional difficulty due to constraints required to yield positive definite estimates make covariance estimation a potentially complex optimization problem. Further, most existing approaches to covariance estimation require data to be sampled at regular grid (time) points, with subjects sharing a set of common observation points. However, in many practical situations, data are irregularly sampled, and subjects may share few common observation times, and methods are needed to accommodate for data collected in this way.  

To address the challenge of enforcing positive definiteness, several have considered modeling various matrix decompositions including variance-correlation decomposition, spectral decomposition, and Cholesky decomposition. The Cholesky decomposition has received particular attention, as it which allows for a statistically meaningful interpretation as well as an unconstrained parameterization of elements of the covariance matrix. This parameterization allows for estimation to be accomplished as simply as in least squares regression. 

[ ADD citation for the first to introduce using the modified cholesky decomposition ]

It is well known that the sample covariance matrix is unstable in high dimensions, and there is an extensive existing body of work addressing the issue of high dimensionality in the context of covariance estimation. [cite the pourahmadi survey paper here.] However, much of this work addresses high dimensionality arising from functional or times series data sampled on a dense, regular grid. With such data, it is typical that the number of time points is larger than the number of observations. Few have addressed the challenges posed by sparse longitudinal data where measurement times may be almost unique yet sparsely distributed within the observed time range for each individual in the study. In this case, high dimensionality may not be a consequence of having more measurements per subject than the number of subjects themselves, but rather because when pooled across subjects, the total number of unique observed time points is greater than the number of individuals. Incomplete and unbalanced data arise when measurement schedules with targeted time points which are not necessarily equally spaced or if there is missing data. Sparse longitudinal data arise when the measurement schedule has arbitrary or almost unique time points for every individual. A given time point may have very few individuals with corresponding measurements. 

We sidestep both issues of high dimensionality and irregularly sampled data by viewing the response as a stochastic process having continuous covariance function. Recent work outlines the use of function estimation for smoothing elements of the covariance matrix, including [cite all the smoothing papers here. ] To our knowledge, however, no previous work has applied bivariate smoothing both dimensions of the Cholesky factor. We model the generalized autoregressive parameters using tensor product splines. Viewing covariance modeling as bivariate function estimation both accommodates irregularly sampled curved and permits interpolation and extrapolation of the covariance function between two measurements at any pair of time points within the time interval of interest rather than at observed pairs of time points only. Through the Cholesky decomposition, we carry out estimation by the estimation of varying coefficient model. A transformation of the design point axes allows for an ANOVA-like decomposition of the coefficient function into two components, corresponding to the lag between time points and an additive component. Through this general framework, we can easily impose penalties on fitted functions to yield natural null models presented in the literature. 


\section{Cholesky Decomposition of $\Sigma$}

To present a comprehensive overview our estimation procedure, we begin with the representation of the inverse covariance matrix, $\Sigma^{-1}$, in terms of its Cholesky decomposition (see citet{pourahmadi2007cholesky} for a detailed discussion.) In the section to follow, we will demonstrate that this parameterization of the precision matrix is particularly attractive due to both the computational advantages as well as the convenient modeling interpretation it permits. For any positive definite matrix $\Sigma$, there exists a unique unit lower triangular matrix $T$ with diagonal entries equal to $1$ which diagonalizes $\Sigma$:

\begin{equation}
\nonumber T \Sigma T^T = D
\end{equation}
\noindent

If we assume that the data having covariance matrix $\Sigma$ follow an autoregressive model, then the entries of the Cholesky factor $T$ and $D$ enjoy a useful interpretation. Let $Y = \left( y_{1}, y_{2}, \dots, y_{m} \right)^T$ denote a mean-zero random vector of observations with corresponding measurement times 

\[
t_{1} < t_{2} < \dots< t_{m}.
\]

Consider regressing $y_{j}$  on its predecessors:

\begin{equation}
{y}_{j}  = \sum_{k=1}^{j-1} \phi_{jk} y_{k} + \sigma_{j}e_{j}, \qquad j=2,\dots,m, \label{eq:ARmodel}
\end{equation}
\noindent where we define $y_{1}=e_{1}$. Standard regression theory gives us that if $\lbrace \phi_{jk} \rbrace$ are the coefficients of the linear least squares predictor of $y_{j}$ based on its predecessors, then the residuals $e =\left( e_{1}, e_{2},\dots, e_{m} \right)^T$ have diagonal covariance. Let $T$ denote the $m \times m$  matrix with elements 

\[
T_{jk} = \left\{
\begin{array}{ll}
-\phi_{jk} & j > k\\
1 & j = k \\
0 & otherwise,
\end{array}\right.
\]
\noindent
for $j,k=1,\dots,m$. Then in matrix notation,  model~\ref{eq:ARmodel} may then be written

\begin{equation}
e = T Y, \label{eq:epsilon}
\end{equation}
\noindent

Taking covariances on both sides of \ref{eq:epsilon}, we have

\begin{equation}
\nonumber
D = T \Sigma T^T
\end{equation} 

 The unconstrained regression coefficients $\lbrace \phi_{jk} \rbrace$ are referred to as the \emph{generalized autoregressive parameters} (GARPs). The $\lbrace \sigma^2_{j} \rbrace$ are called the \emph{innovation variances} (IVs.)  Unconstrained estimation of the $\left\{ \sigma_{k}^2 \right\}$ is achieved by log transformation;  we leave these details for section 2. Expressing the precision matrix  in terms of the GARPs and IVs, we have
\begin{equation} \label{eq:omega_decomp}
\Omega= \Sigma^{-1} = T^T D^{-1} T.
\end{equation}

Rather than estimating a specific covariance matrix for data observed on a fixed, regular grid, we aim to estimate a smooth covariance function. This accomodates data which may consist of  observations on multiple subjects measured at potentially unequally spaced and individual-specific times. Rather than $m$-dimensional vectors, consider $Y$ and $e$ as the values of the stochastic processes $Y\left(t\right)$ and $e\left(t\right)$ at the set of observation times.  We assume that $Y\left(t\right)$ is equipped with covariance function $G\left(s,t\right)$, and
\[
e\left(s\right) \sim \mathcal{WN}\left(0,1\right)
\] 
is a zero mean Gaussian white noise process with unit variance. We assume that $G\left(s,t\right)$ satisfies some smoothness conditions, where smoothness is defined in terms of square integrability of certain derivatives. [TODO: clean up statement about smoothness of covariance function; integrability of covariance function of a stochastic process?] The entries of $\Sigma$, then, correspond to $G$ evaluated at the distinct pairs of observed time points. Similarly, we treat the elements of the precision matrix $\Omega$ as the values of some smooth function, $\omega\left(s,t\right)$ evaluated at observed pairs of time points.

Extending this perspective to the elements of $D$ and the elements of the Cholesky factor $T$ leads us to  %view the GARPs $\lbrace \phi_{ijk} \rbrace$ and innovation variances as the evaluation of the smooth functions $\phi\left(s,t\right)$ and $\sigma^2\left(t\right)$ at observed time points and interpret $\phi_{ijk} = \phi\left(t_{ij},t_{ik}\right)$ and $\sigma_{ij}^2 = \sigma^2\left(t_{ij}\right)$. 
the varying coefficient (VC) models first introduced by Hastie and Tibshirani. A generalization of traditional linear regression models, varying coefficient models offer more flexibility than their static analogues by allowing the effect of covariates to change smoothly with the value other variables. Both regressors and response variables are assumed to vary according to an \emph{indexing variable}, which is particularly attractive because this permits interpolation of regressors and response variables at values of this indexing variable where there is either missing data of only a single observation and slope estimation is not feasible.

Replacing  $\left \{ \phi_{jk} \right\}$ and $\left\{ \sigma_j \right\}$ with smooth functions, we model 

\begin{equation}   
y\left(t_j \right)  = \sum_{k=1}^{j-1} \phi\left(t_j ,t_k\right) y\left(t_k\right) + \sigma\left(t_j\right)\epsilon\left({t_j}\right) \;\;\;\; j=1,\dots, m, 
\label{eq:MyModel} 
\end{equation}

\noindent for $t_1 < t_2 < \dots < t_m$. 

We propose the smoothing splines of [cite craven and wahba] as well as the penalized B-splines proposed by Marx and Eiler for estimation of $\phi\left(s,t\right)$ and $\sigma^2\left(\right)$ alongside novel penalties to induce smoothness and parsimony in the estimated functions.  For ease of exposition, we first focus our attention on the estimation of $\phi$ assume that $\sigma^2\left(t\right)$ is fixed and known; we will later relax this assumption and discuss the estimation of $\sigma^2$ and $\phi$ simultaneously. Assumptions about the functional nature of $\phi$ drive the choice of the penalty; suggested choices are discussed in sections [INSERT PENALTY SECTIONS HERE].   %square integrability of certain derivatives. 
Recasting the problem as the estimation of model~\ref{eq:MyModel} allows us access to the existing set of tools developed in the bivariate smoothing literature; our approach provides a flexible, comprehensive framework for covariance estimation.
%Observed time points may be individual-specific and not necessarily on a regular grid. 

\section{Penalized Maximum Likelihood Estimation of $\phi$}

Along with citet{huang2006covariance}, citet{levina2008sparse}, and citet{pourahmadi2000maximum} we employ the Gaussian log-likelihood as a goodness of fit measure for the varying  autoregressive coefficient function, $\phi\left(t,s\right)$ and the innovation variance function $\sigma\left(t\right)$, though neither the derivation of model~\ref{eq:ARmodel} nor model~\ref{eq:MyModel} rely on any assumptions about the distribution of $e$. Fixing $\sigma_j^2$, the negative loglikelihood as a function of $\phi_{jk}$ corresponds to the usual error sums of squares.  Under the gaussian assumption, for fixed $\left\{ \sigma_j^2 \right\}$, the negative log-likelihood of $N$ i.i.d. vectors of observations $y_1,y_2,\dots,y_N$ is proportional to

\begin{equation}
-2 L\left(y_1, y_2, \dots,y_N ,\Phi \right) \propto \sum_{i=1}^N \sum_{j=2}^{m_i} \sigma\left({t_j}\right)^{-2} \left(y_{ij} - \sum_{k=1}^{j-1}\phi\left({t_{ij},t_{ik}}\right)y_{ik} \right)^2 \label{loglikelihood}
\end{equation}
\noindent
where 
\[
y_i = \left( y_{i1}, y_{i2}, \dots, y_{i,m_i}\right), \quad i=1,\dots,N 
\] 

%% explain here that we omit y_{i1} from the likelihood here because in the first stage of estimation (for $\phi$), the first observation in each vector doesn't contribute to the likelihood
\noindent
denotes the vector of observations for subject $i$ with corresponding measurement times 
\[
t_{i1} < t_{i2} < \dots < t_{i,m_i}.
\]
The form of the likelihood of $y_1,\dots,y_N$ indicates that we allow both the number of measurements as well as the observation times to varying across subjects. The $\left\{t_{ij} \right\}$ need not be evenly-spaced within or across individuals.  In the case that subjects share a common set of observation times $t_1 < \dots < t_m$,  it is well known that the MLE for $\Sigma$, $S = \sum_{i=1}^N y_i y_i^T$ is highly unstable in high dimensions. [cite those who have proposed mitigating this using penalized maximum likelihood for the distinct elements of T.] This condition is potentially worsened when one or more subjects has at least one unique observation time. To mitigate instability due to high dimensionality and simultaneously permit the estimation of $\phi\left(\cdot,\cdot\right)$ as a smooth bivariate function, we obtain a covariance estimator by applying bivariate smoothing of the elements of the Cholesky factor. We impose two families of penalties on fitted functions, leading to two distinct parameterizations of the smoothed function [blah blah something about flexibility and penalty being problem-specific.]  


\section{Representation of $\phi$ as a smooth function}

To impose structure on the estimated autoregressive function, we append a penalty functional to the negative log-likelihood \ref{eq:}. that discourages the flexibility of the fitted function. We take the estimator of $\phi$ to minimize

%\begin{itemize}
%\item {\bf Step 1:} Select $\hat{\lambda}_1$, where
\begin{equation} 
 -2 L + \lambda J_{\phi}\left(\phi\right)  \label{eq:phi-objective-fnc}.
\end{equation}
%\item {\bf Step 2:} Define $\hat{\phi}$ to the the minimizer of 

The first term in \ref{eq:phi-objective-fnc} discourages the lack of fit of $\phi$ to the data, and $\lambda$ is a smoothing parameter which controls the tradeoff between the lack of fit and amount of regularization imposed on the fitted function through the penalty, $J_\phi$.The task of estimating of $\phi\left(t,s\right)$ is an inherently different problem than the estimation of an arbitrary bivariate function. Since $\phi$ explicitly defines an inverse covariance function, imposing specific types of structure on $\phi$ is of particular interest. Covariance models for longitudinal or time series data are commonly defined in terms of lag, or in the continuous case, the difference between two measurement times.  To express the coefficient function in terms of functional components reflecting this particular interest, we transform the original input axis, letting  
\begin{align*}
l &= s-t \\
m &= \frac{1}{2}\left(s+t\right).
\end{align*}
\noindent
Writing $\phi$ in terms of the rotation gives the reparameterized coefficient function 
\begin{equation}
\phi^*\left(l,m\right) = \phi^*\left(s-t, \frac{1}{2}\left(s+t\right)\right) = \phi\left(s,t\right).
\end{equation}

We define our estimator $\hat{\phi^*}$ as the minimizer of

\begin{equation} 
-2 L + \lambda^* J_{\phi^*}\left(\phi^*\right) \label{eq:phi-star-objective-fnc}.
\end{equation}

\subsection{Smoothing spline ANOVA models}

We consider models that capture the marginal effects of $l$ and $m$, as well as interaction between the two directions. We first consider the smoothing spline ANOVA decomposition of citet{gu2002smoothing},  modeling 

\begin{equation}
\phi^*\left(l,m\right) = \mu + \phi_l\left(l\right) + \phi_m\left(m\right) + \phi_{lm}\left(l,m\right).   \label{eq:ANOVA}
\end{equation}
\noindent 

As in cite{gu2002smoothing}, cite{craven1978smoothing},[ more Wahba citations here ], we consider functions $\phi^*$ belonging to a reproducing kernel Hilbert space (r.k.h.s.), $\mathcal{H}$. We equip each $l$ and $m$  with corresponding univariate Hilbert spaces, $\mathcal{H}_l$ and $\mathcal{H}_m$, choosing to let $\mathcal{H}_l$ correspond to the second-order Sobolev space $W_2\left(0,1\right)$ and $\mathcal{H}_m$ to the first-order Sobolev space $W_1\left(0,1\right)$, where
\[
W_m\left(0,1\right) = \lbrace f: \;\;f, f^\prime \mbox{absolutely continuous}, \int_0^1 \left(f^{\left( m \right)}\right)^2 dt < \infty \rbrace.
\]
\noindent
for $m=1, 2$. Each space $\mathcal{H}_l$, $\mathcal{H}_m$ is endowed with inner product  product

\begin{equation} \label{eq:inner_product}
\big < f, g \big > = \sum_{\nu=0}^{m-1} \left( \int_0^1 f^{\left( \nu \right)}\left(x\right) dx \right)\left( \int_0^1 g^{\left( \nu \right)}\left(x\right) dx \right) + \int_0^1 f^{\left( m \right)} g^{\left( m \right)}dx
\end{equation}

 The space of bivariate functions $\mathcal{H}$ can be constructed from the tensor product of the univariate function spaces for $l$ and $m$:

\[
\mathcal{H} = \mathcal{H}_l \otimes \mathcal{H}_m.
\]
\noindent




Several have proposed methods for applying regularization of Cholesky decomposition including banding, tapering, kernel smoothing, penalized likelihood, and penalized regression. See [  ] Within the function estimation paradigm, a number of  approaches to estimate the coefficient function $\phi\left(\cdot,\cdot\right)$ have been proposed including See cite{wu2003nonparametric},  cite{huang2007estimation} .  Common techniques for inducing structure to produce simple and stable covariance estimates include shrinking estimated functions or the elements of the covariance matrix itself so that the resulting dependency structure corresponds to parsimonious covariance models frequently adopted in the time series and longitudinal  data literature.
[
CITE PAPERS PROPOSING PARSIMONIOUS MODELS FOR phi ij
]
The ANOVA model in \ref{eq:ANOVA} allows us to easily specify penalties $J$ that encourage estimates to adhere to the structure of these models.   [cite some general time series/longitudinal sources ] When $\phi^*$ corresponds to the simple models of the form \eqref{covmodel}, the bivariate function may be written in terms of only its first argument. 
. . %cite{wu2003nonparametric}, for example, used locally weighted polynomials to smooth down the sub-diagonals of $T$. cite{huang2007estimation} smoothed the sub-diagonals of $T$ using univariate smoothing splines. 

The penalty functional $J$ induces a decomposition of $\mathcal{H}$ as a direct sum of two subspaces: 

\[
\mathcal{H} = \mathcal{H}_0 \oplus \mathcal{H}_1,
\]
\noindent
where $\mathcal{H}_0$ denotes the null space of $J$, spanned by $\tau_1, \tau_2, \dots \tau_M$, and $\mathcal{H}_1$ is the subspace orthogonal to $\mathcal{H}_0$. Let $P_1 \phi^*$  denote the projection of $\phi^*$ onto the penalized space $\mathcal{H}_1$. We can express $J$ in terms of the projection of $\phi^* \in \mathcal{H}$ onto $\mathcal{H}_1$:
\begin{align}
\begin{split} 
J\left(\phi\right) &= \vert \vert P_1 \phi^* \vert \vert^2\\
&= \vert \vert {P_1 \phi_l} \vert \vert^2 + \vert \vert {P_1 \phi_m} \vert \vert^2 + \vert \vert {P_1 \phi_{lm}} \vert \vert^2  \label{eq:SS_penalty}
\end{split}
\end{align} 
\noindent

The decomposition of $\mathcal{H} = \mathcal{H}_0 \oplus\mathcal{H}_1$ can be characterized by the decompositions of $\mathcal{H}_l$ and $\mathcal{H}_m$ induced by $J$:

\begin{align}
\begin{split} \label{eq:}
\mathcal{H}_l &= \mathcal{H}_{l0} \oplus \mathcal{H}_{l1}\\
\mathcal{H}_m &= \mathcal{H}_{m0} \oplus \mathcal{H}_{m1}
\end{split}
\end{align}
\noindent

As $\lambda \rightarrow \infty$, the penalty term dominates the objective function in \ref{eq:phi-star-objective-fnc}, forcing the minimizer to adopt the functional form of the $\mathcal{H}_0$. The parameterization in \ref{eq:ANOVA} allows us to easily construct penalties to so that for large values of $\lambda$, the fitted function will correspond to [cite the simple parametric and semiparametric models of Pourahmadi, Wu, etc as well as the null models proposed by others utilizing smoothing methods]. We consider specification of the penalty so that the null space excludes any functions $\phi^*$ which are non-constant in $m$, letting 

\begin{equation} \label{eq:null_space_m}
\mathcal{H}_{m0} =  \lbrace  1 \rbrace
\end{equation}
\noindent
 Additionally, we let $\phi^*$ which are linear in lag $l$ to incur zero penalty, letting 

\begin{equation} \label{eq:null_space_l}
\mathcal{H}_{l0} =  \lbrace 1 \rbrace \oplus \lbrace k_1 \rbrace,\\
\end{equation}
\noindent
where $k_\nu = B_\nu/\nu!$ are scaled Bernoulli polynomials satisfying 

\begin{align*}
B_0\left(x\right) &= 1,\\
\frac{d}{dx} B_j\left(x\right) &= jB_{j-1}\left(x\right).
\end{align*}

The penalized spaces $\mathcal{H}_{l1}$, $\mathcal{H}_{m1}$, defined as the subspaces orthogonal to $\mathcal{H}_{l0}$ and $\mathcal{H}_{m0}$  respectively, satisfy
\begin{eqnarray*}
\mathcal{H}_{l1} = \lbrace \phi_l: \int_0^1 {\phi_l}^{\left( \nu \right)}\left(l\right) dl = 0,\;\; \nu = 0,1\rbrace\\
\mathcal{H}_{m1} = \lbrace \phi_m: \int_0^1 \phi_m \left(m\right) dm = 0 \rbrace\\
\end{eqnarray*}
\noindent
Using the properties of tensor product spaces, we may write $\mathcal{H}_0$ and $\mathcal{H}_1$ in terms of the elements defining the marginal subspaces:

\begin{align*}
\mathcal{H}_0 &= \lbrace 1 \rbrace \oplus \lbrace k_1 \rbrace\\
\mathcal{H}_1 &= \mathcal{H}_{l1} \oplus \mathcal{H}_{m1} \oplus  \left[ \lbrace k_1 \rbrace  \otimes  \mathcal{H}_{m1} \right]  \oplus  \left[\mathcal{H}_{l1} \otimes  \mathcal{H}_{m1}.\right]   
\end{align*}

\begin{minipage}[h]{\linewidth}
\vspace{0.5in}
\centering
\captionof{table}{Tensor product space $\mathcal{H}$} \label{tab:tensor_product_subspaces} 
\begin{tabularx}{\linewidth}{@{} C{1in} | C{.85in} *4X @{}}\toprule[1.5pt]
 &  $\left\{ 1 \right\}$ & $ \left\{ k_1\left( l \right) \right\}$  & $\left\{ \mathcal{H}_{l1} \right\}$ \\
\midrule
$\left\{ 1 \right\}$   & $\left\{ 1 \right\}$   &  $\left\{ k_1\left( l \right) \right\}$ & $\left\{ \mathcal{H}_{l1} \right\}$ \\
&&&\\
$\left\{ \mathcal{H}_{m1} \right\}$   & $\left\{ \mathcal{H}_{m1} \right\}$  & $ \left\{ \mathcal{H}_{m1} \right\} \otimes \left\{ k_1\left( l \right) \right\}$   &	$\left\{ \mathcal{H}_{m1} \right\} \otimes \left\{ \mathcal{H}_{l1} \right\}$
\end{tabularx} \par
\bigskip
The subspaces of $W_1\left[ 0,1 \right] \otimes W_2\left[ 0,1 \right]$ by the tensor product of the marginal subspaces of $\mathcal{H}_l$, $\mathcal{H}_m$.
\end{minipage}
\vspace{0.5in}

Table~\ref{tab:tensor_product_subspaces} shows how the space of two-dimensional functions is constructed by taking tensor products of each of the subspaces which define the two univariate spaces, $\mathcal{H}_l$ and $\mathcal{H}_m$.  One may show that the reproducing kernels for $\mathcal{H}_{l0}$ and $\mathcal{H}_{l1}$ are given by $R^0_{l}\left(l,l^\prime\right) = \sum_{\nu=0}^1 k_\nu\left(l\right)k_\nu\left(l^\prime\right)$ and $R_l^1\left(l,l^\prime\right) = k_2\left(l \right)k_2\left(l^\prime \right) - k_{4}\left(\left[ l-l^\prime \right] \right)$, respectively, where $\left[ z \right]$ denotes the integer part of $z \in \R$. The reproducing kernel for the full marginal space $\mathcal{H}_l$ is simply the sum of the reproducing kernels for each of the subspaces: 
\[
R_l\left(l,l^\prime\right) = \sum_{\nu=0}^1 k_\nu\left(l\right)k_\nu\left(l^\prime\right) + k_2\left(l \right)k_2\left(l^\prime \right) - k_{4}\left(\left[ l-l^\prime \right] \right).
\]
\noindent
One can also show that the reproducing kernels for $\mathcal{H}_{m0}$ and $\mathcal{H}_{m1}$ are given by  $R^0_{m}\left(m,m^\prime\right) = 1$ and $R_m^1\left(m,m^\prime\right) = k_1\left(m \right)k_1\left(m^\prime \right) + k_2\left(m \right)k_2\left(m^\prime \right) - k_{4}\left(\left[ m-m^\prime \right] \right)$, and similarly, the reproducing kernel $R_m$ for the full marginal space $\mathcal{H}_m$ is taken to be the sum of the kernels for each subspace. The tensor product reproducing kernel for $W_1\left[ 0,1 \right] \otimes W_2\left[ 0,1 \right]$ is obtained by simply taking the product of each of the reproducing kernels for the univariate function spaces:
\[
R\left( \left(l,l^\prime \right), \left(m,m^\prime\right) \right)
\]
\noindent
Table~\ref{tab:tensor_product_RK} shows the decomposition of the reproducing kernel for the space of bivariate functions into the product of the reproducing kernels for each of the univariate subspaces. 

\begin{minipage}[h]{\linewidth}
\vspace{0.5in}
\centering
\captionof{table}{Tensor product reproducing kernel $R\left(\left(l,m\right),\left(l^\prime,m^\prime\right)\right)$} \label{tab:tensor_product_RK} 
\begin{tabularx}{\linewidth}{@{} C{1in} | C{.85in} *4X @{}}\toprule[1.5pt]
			 &  $\left\{ 1 \right\}$ & $ \left\{ k_1\left( l \right) \right\}$  			& $\left\{ \mathcal{H}_{l1} \right\}$ \\
\midrule
$\left\{ 1 \right\}$   & $\left\{ 1 \right\}$   &  $ k_1\left( l \right) k_1\left( l^\prime \right) $	 & 	$ R^l_{1}$ \\
&&&\\
$\left\{ \mathcal{H}_{m1} \right\}$   & 	 $R^1_{m}\left(m,m^\prime\right)$  &  $R^1_{m}\left(m,m^\prime\right)  k_1\left( l \right) k_1\left( l^\prime \right) $  &   $R^1_{m}\left(m,m^\prime\right)R^1_{l}\left(l,l^\prime\right)$
\end{tabularx} \par
\bigskip
%The  reproducing kernel for each of the subspaces of $\mathcal{H}$.
\end{minipage}
\vspace{0.5in}


For $\mathcal{H}$ defined as above, our goal is to find $\phi^* \in \mathcal{H}$  that minimizes 

\begin{equation} \label{eq:penalized_loglikelihood}
-2L + \lambda \vert \vert P_1 \phi^* \vert \vert^2 =  \sum_{i=1}^N \sum_{j=2}^{m_i} \sigma\left({t_j}\right)^{-2} \left(y_{ij} - \sum_{k=1}^{j-1}\phi^*\left({l_{ijk},m_{ijk}}\right)y_{ik} \right)^2 + \lambda  \vert \vert P_1 \phi^* \vert \vert^2
\end{equation}
\noindent
where $\left(l_{ijk}, m_{ijk}\right)$ are the result of transforming subject $i$'s $j$ and $k^{th}$ observation times, $t_{ij}$ and $t_{ik}$.  Let  $B$ denote the $p \times M$  matrix  with columns correponding to the $M$ basis functions spanning $\mathcal{H}_0$ evaluated at the $p = \sum_i=1^N {n_i \choose 2}$ within-subject pairs of observed time points.  In spirit of the result on the representation of a univariate $f \mathcal{H}$ given by cite{kimeldorf1971some}, we can similarly show that the minimizer of \ref{eq:penalized_loglikelihood} lies within a finite dimensional space despite the minimization being carried out over an infinite dimensional space, $\mathcal{H}$. 

\begin{theorem} \label{phi_representer_thm}
Let $p= \sum_{i=1}^N {n_i \choose 2}$ be the total number of distinct within-subject pairs of design points, and index the transformed pairs $\left( l,m \right)_i$, $i=1, \dots,p$. Let $B$ be the $p \times 2$ matrix with $\left(i,j\right)^{th}$ entry $k_j\left( \left( l,m \right)_i \right)$ with rank $r=2$. Then, the unique minimizer of the penalized likelihood \eqref{penlik1}, $\phi^* \in \mathcal{H}$ is of the form

\begin{equation}
\phi^*\left(l,m\right) = d_0 + d_1 k_1\left(l\right) + \sum_{i=1}^{p} c_i R^1\left( \left(l,m\right) , \left(l,m \right)_i\right)
 \label{eq:finitedimsolution}
\end{equation}
\end{theorem}

The proof is left to the appendix; see also scholkopf2001generalized for a generalization of the result in cite{kimeldorf1971some} which relaxes the assumption of squared-error loss. 

\subsection{Demonstration of the smoothing spline fitting procedure: Examples}


\section{ Optimal shrinkage of $T$: smoothness may not be enough }


The varying coefficient function $\phi^*$ is quite different from the usual problem of estimating an arbitrary bivariate function via smoothing. In the case of the latter, we most typically treat both arguments equally in terms of regularization, but in the case of covariance estimation and the generalized coefficient function equal treatment of $l$ and $m$ in terms of penalization perhaps is not the most appropriate approach. The lag component, $l$, has particularly significant meaning in terms of the covariance function and thus also in terms of $\phi^*$ and is of considerable more interest than the orthogonal component, $m$. As discussed in Section 2, we can define an entire class of functional autoregressive models using only the $l$ direction, and additionally, as discussed in Section 3, there is a natural expectation about the functional form of the autoregressive coefficient function (and hence covariance) as a function of $l$, making imposing that conditional dependence between observation decay as $l$ and the time between observations increase a reasonable way to institute regularization.


\subsection{The truncated power basis and an alternative decomposition of $\mathcal{H}$}


cite{bickel2008regularized} discuss 
2004; 2008a; Wu and Pourahmadi, 2009) and generally those based on the Cholesky decomposition
of the covariance matrix or its inverse (Pourahmadi, 1999, 2000; Rothman et al.
2010) do impose an order among the components of Y and are not permutation-invariant.

The parameters of the functional autoregressive model given by \ref{eq:MyModel} define the elements of the precision matrix $\Omega$, rather than the elements of $\Sigma$ itself. It is well known that if we let $Y = \left(Y_1, \dots, Y_m\right)^T$ denote the random vector having joint distribution with mean zero and covariance matrix $\Sigma$, then the elements of $\Sigma^{-1}=\Omega$, $\left\{ \omega_{ij} \right\}$ may be interpreted as partial covariances between the elements of $Y$.  [ reference proposals for banding the cholesky factor, either by manual truncation or by nested lasso penalty ] This suggests shrinking $\phi^*$ to zero for large values of $l$. One can show that if $T$ has $k$ non-zero diagonals, then the middle $k$ diagonals of $\Sigma^{-1}$ are non-zero.  The procedure outlined in Section~\ref{} yields null models that are linear in $l$, and are not necessarily monotonic. 

%citet{chen2011efficient}, citet{pourahmadi1999joint}, and citet{pourahmadi2002dynamic} have elicited parametric models for the generalized autoregressive coefficients, letting the GARPs depend only on the distance between two time points.

This latter notion is instrumental in justifying the family of penalties
\[
J_{2,\left(p\right)} = \sum_{  l_i \in \mathcal{L}: l_i > l_0} \vert \mu^* + \phi^*_1\left(l_i\right) \vert^p 
\]
\noindent
which we may view as a design-driven way of implementing the regularization which may be imposed by the penalty functionals taking the form

\begin{eqnarray} \nonumber
J\left(\phi^*\right) &=& \int_{l_0}^1 \vert \mu^* + \phi^*_1\left(l\right) \vert^p\; dl\\
&=& \int_{0}^1 \vert \mu^* + \phi^*_1\left(l\right) \vert^p I\left(l > l_0\right) \; dl \label{eq:truncated_penalty}
\end{eqnarray}

The penalty functionals given by \eqref{truncated_penalty} motivate a different decomposition of $\mathcal{H}$ than the derivative-based penalty. The form of \eqref{truncated_penalty} is significantly different in nature from the penalty discussed in Section 2.1 and those typically encountered in the setting smoothing spline ANOVA models, particularly because \eqref{truncated_penalty} effects only a subset of the domain for $l$. Therefore, an appropriate decomposition of the function space into the null space of $J$ and the penalized space should perhaps be formulated in terms of basis functions for the lag component, $l$ with domains which do not include the entire unit interval. 

The truncated power basis, as in their use in defining polynomial regression splines, enjoy a particular ease of interpretation, as the coefficient $\beta_{i+k}$ may be identified as the size of the jump at $x_i$ in the $k^{th}$ derivative of $f$. This fact is especially useful when tracking change points or, in general, any abrupt changes in the regression curve. If we reflect these basis functions about each of their corresponding knot points and denote these reflections $\lbrace T^-_{ik}\rbrace$, then expressing the regularization corresponding to the penalty functionals \eqref{truncated_penalty} becomes quite natural in terms of the reflected basis functions $\left(\cdot - l_1 \right)^k_-,\dots, \left(\cdot - l_n \right)^k_-$, where $\left( \alpha \right)_- = \max\left(-\alpha,0\right)$.  While the truncated power basis initially appears very attractive for representing functions in terms of the decomposition induced by penalties of the same form as that in Equation~\ref{eq:truncated_penalty}, they 

\section{P-splines}

\subsection{Truncated Power Basis}

\subsection{B-spline Basis}
\subsection{Difference penalties}






\section{Appendix}


Proof of Theorem~\ref{phi_representer_thm}"

Then we may verify that any $\phi^* \in \mathcal{H}$ can be written 
\[
\phi^*\left(l,m \right) = d_0 + d_1k_1\left(l\right) + \sum_{i=1}^n  c_i R_1\left( \left(l,m\right) , \left(l_i,m_i \right)\right) + \rho\left(l,m\right)
\]
\noindent
where $\rho \perp \mathcal{H}_0 = \lbrace 1\rbrace \oplus \lbrace k_1\rbrace,\; span\lbrace R_1\left(\left(l_i, m_i \right),\cdot \right)  \rbrace$. We do so by demonstrating that  $\rho$ does not improve the first term in \eqref{eq:objectivefun} (the data fit functional) and only adds to the penalty term, $J\left(\phi^*\right)$. Consequently, if $\hat{\phi^*}$ is the minimizer of \eqref{eq:objectivefun}, then $\rho = 0$. Using the properties of reproducing kernels, we can rewrite $\phi^*$ as an inner product of itself with $R$:
 
\begin{eqnarray*}
\phi^*\left(l_j,m_j \right)  &=& \left< R\left(\left(l_j,m_j\right),\left(\cdot,\cdot\right) \right),\phi^*\left(\cdot,\cdot\right)\right>\\
&=& \left<R_0\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right) + R_1\left(\left(l_j,m_j\right),\left(\cdot,\cdot\right) \right),d_0 + d_1k_1\left(\cdot \right)\right. \\ 
&\mbox{ }&\left. \;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;\;+ \sum_{i=1}^{N_{\phi^*}}  c_i R_1\left( \left(l_i,m_i \right),\left(\cdot,\cdot\right) \right) + \rho\left(\left(\cdot,\cdot \right)\right)\right>\\
&=& \left<R_0\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right) , d_0 + d_1k_1\left(\cdot\right)\right> + \left< R_0\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right),\sum_{i=1}^{N_{\phi^*}}  c_i R_1\left( \left(l_i,m_i \right),\left(\cdot,\cdot\right) \right)\right> \\
&\mbox{ }& + \left<R_0\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right), \rho\left(\left(\cdot,\cdot \right)\right)\right> + \left<R_1\left(\left(l_j,m_j\right),\left(\cdot,\cdot\right) \right), d_0 + d_1k_1\left(\cdot \right)\right> \\
&\mbox{ }& + \left<R_1\left(\left(l_j,m_j\right),\left(\cdot,\cdot\right) \right),\sum_{i=1}^{N_{\phi^*}}  c_i R_1\left( \left(l_i,m_i \right),\left(\cdot,\cdot\right) \right) \right> + \left<R_1\left(\left(l_j,m_j\right),\left(\cdot,\cdot\right) \right), \rho\left(\left(\cdot,\cdot \right)\right)\right>\\
&=& \left<R_0\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right) , d_0 + d_1k_1\left(\cdot\right)\right> + \left<R_1\left(\left(l_j,m_j\right),\left(\cdot,\cdot\right) \right),\sum_{i=1}^{N_{\phi^*}}  c_i R_1\left( \left(l_i,m_i \right),\left(\cdot,\cdot\right) \right) \right> \\
&\mbox{ }& + \underbrace{\left<R_0\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right)  , \rho\left(\cdot,\cdot\right) \right>}_{0} + \underbrace{\left<R_1\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right)  , \rho\left(\cdot,\cdot\right) \right>}_{0}\\
&=& d_0 + d_1k_1\left(\cdot \right) + \sum_{i=1}^{N_{\phi^*}}  c_i R_1\left( \left(l_i,m_i \right),\left(l_j,m_j\right) \right)
\end{eqnarray*}
\noindent


Rewriting the data fit functional, we have that  
 \begin{eqnarray*}
&\mbox{ }&\sum_{i=1}^N \sum_{j=1}^{n_i} \sigma_{ij}^{-2} \left(y\left(t_{ij}\right) - \sum_{k=1}^{j-1} \phi^*\left(t_{ij}, t_{ik}  \right) y\left(t_{ik}\right)  \right)^2  \\ 
&=& \sum_{i=1}^N \sum_{j=1}^{n_i} \sigma_{ij}^{-2} \left(y\left(t_{ij}\right) - \sum_{k=1}^{j-1} \left< R\left(\left(l^i_{jk},m^i_{jk}\right),\left(\cdot,\cdot\right) \right),\phi^*\left(\cdot,\cdot\right)\right> y\left(t_{ik}\right)  \right)^2  \\
 \end{eqnarray*}
\noindent
which is free of $\rho$. Consider the contribution of any nonzero $\rho$ to $J\left(\phi^*\right)$: 
  
 \begin{eqnarray*}
 J\left(\phi^*\right) &=& \vert \vert  P_1\phi^* \vert \vert^2\\
 &=& \left< \sum_{i=1}^{N_{\phi^*}}  c_i R_1\left( \left(l_i,m_i\right),\left(\cdot,\cdot\right) \right) + \rho\left(\cdot,\cdot \right), \sum_{j=1}^{N_{\phi^*}} c_j R_1\left( \left(l_j,m_j\right),\left(\cdot,\cdot\right) \right) + \rho\left(\cdot,\cdot\right)\right> \\
 &=& \vert \vert \sum_{i=1}^{N_{\phi^*}}  c_i R_1\left(\left(l_i,m_i\right),\left(\cdot,\cdot\right) \right) \vert \vert^2 + \vert \vert  \rho \vert \vert^2 
 \end{eqnarray*}
\noindent
Thus, including $\rho$ in $\phi^*$ only increases the penalty without improving (decreasing) the data fit functional, so we indeed have that the minimizer of \eqref{eq:objectivefun} has the form
\begin{equation}
 \phi^*\left(l,m\right) =  d_0 + d_1k_1\left(l\right) + \sum_{i=1}^{N_{\phi^*}} c_i R_1\left( \left(l,m\right) , \left(l_i,m_i \right)\right)
 \label{eq:finitedimsolution}
 \end{equation}

\end{document}
