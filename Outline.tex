\documentclass[12pt]{article}
\usepackage{graphicx,psfrag,amsfonts,float,mathbbol,xcolor,cleveref}
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage[mathscr]{euscript}
\usepackage{enumitem}
\usepackage{accents}
\usepackage{framed}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{mathtools}
\usepackage{IEEEtrantools}
\usepackage{times}
\usepackage{cite}
\usepackage{amsthm}
\usepackage[letterpaper, left=1in, top=1in, right=1in, bottom=1in,nohead,includefoot, verbose, ignoremp]{geometry}
\newcommand{\comment}[1]{\text{\phantom{(#1)}} \tag{#1}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand*\needsparaphrased{\color{red}}
\newcommand*\needscited{\color{orange}}
\newcommand*\needsproof{\color{blue}}
\newcommand*\outlineskeleton{\color{green}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\bfeps}{\mbox{\boldmath $\epsilon$}}
\newcommand{\bfgamma}{\mbox{\boldmath $\gamma$}}
\newcommand{\bflam}{\mbox{\boldmath $\lambda$}}
\newcommand{\bfphi}{\mbox{\boldmath $\phi$}}
\newcommand{\bfsigma}{\mbox{\boldmath $\sigma$}}
\newcommand{\bfbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\bfalpha}{\mbox{\boldmath $\alpha$}}
\newcommand{\bfe}{\mbox{\boldmath $e$}}
\newcommand{\bff}{\mbox{\boldmath $f$}}
\newcommand{\bfone}{\mbox{\boldmath $1$}}
\newcommand{\bft}{\mbox{\boldmath $t$}}
\newcommand{\bfo}{\mbox{\boldmath $0$}}
\newcommand{\bfO}{\mbox{\boldmath $O$}}
\newcommand{\bfx}{\mbox{\boldmath $x$}}
\newcommand{\bfX}{\mbox{\boldmath $X$}}
\newcommand{\bfz}{\mbox{\boldmath $z$}}


\newcommand{\bfm}{\mbox{\boldmath $m}}
\newcommand{\bfy}{\mbox{\boldmath $y$}}
\newcommand{\bfa}{\mbox{\boldmath $a$}}
\newcommand{\bfb}{\mbox{\boldmath $b$}}
\newcommand{\bfY}{\mbox{\boldmath $Y$}}
\newcommand{\bfS}{\mbox{\boldmath $S$}}
\newcommand{\bfZ}{\mbox{\boldmath $Z$}}
\newcommand{\cardT}{\vert \mathcal{T} \vert}
%\newenvironment{theorem}[1][Theorem]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{corollary}[1][Corollary]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{proposition}[1][Proposition]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{definition}[1][Definition]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\def\bL{\mathbf{L}}


\makeatletter
\renewcommand{\theenumi}{\Roman{enumi}}
\renewcommand{\labelenumi}{\theenumi.}
\renewcommand{\theenumii}{\Alph{enumii}}
\renewcommand{\labelenumii}{\theenumii.}
\renewcommand{\p@enumii}{\theenumi.}
\makeatother

\begin{document}

%\nocite{*}
\def\bL{\mathbf{L}}
%\usepackage{mathtime}

%%UNCOMMENT following line if you have package


\title{ Nonparametric Covariance Estimation for Longitudinal Data via Penalized Tensor Product Splines}

\author{Tayler A. Blake\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201} \and  Yoonkyung Lee\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201}}

\bibliographystyle{plainnat}
\maketitle

\begin{abstract}
With high dimensional longitudinal and functional data becoming much more common, there is a strong need for methods of estimating large covariance matrices. Estimation is made difficult  by the instability of sample covariance matrices in high dimensions and a positive-definite constraint we desire to impose on estimates. A Cholesky decomposition of the covariance matrix allows for parameter estimation via unconstrained optimization as well as a statistically meaningful interpretation of the parameter estimates. Regularization improves stability of covariance estimates in high dimensions, as well as in the case where functional data are sparse and individual curves are sampled at different and possibly unequally spaced time points. By viewing the entries of the covariance matrix as the evaluation of a continuous bivariate function at the pairs of observed time points, we treat covariance estimation as bivariate smoothing. 

Within regularization framework, we propose novel covariance penalties which are designed to yield natural null models presented in the literature for stationarity or short-term dependence. These penalties are expressed in terms of variation in continuous time lag and its orthogonal complement. We present numerical results and data analysis to illustrate the utility of the proposed method. \\
\\
%\begin{keywords}
{\bf keywords:} non-parametric, covariance, longitudinal data, functional data, splines, reproducing kernel Hilbert space
%\end{keywords}
\end{abstract}

\section{Introduction}

An estimate of the covariance matrix or its inverse is required for nearly all statistical procedures in classical multivariate data analysis, time series analysis, spatial statistics and, more recently, the growing field of statistical learning. Covariance estimates play a critical role in the performance of techniques for clustering and classification such as linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), factor analysis, and principal components analysis (PCA), analysis of conditional independence through graphical models, classical multivariate regression, prediction, and Kriging. Covariance estimation with high dimensional data has has recently gained growing interest; it is generally recognized that there are two primary hurdles responsible for the difficulty in covariance estimation: the instability of sample covariance matrices in high dimensions and a positive-definite constraint we wish estimates to obey.

Prevalent technological advances in industry and many areas of science make high dimensional longitudinal and functional data a common occurrence, arising in numerous areas including medicine, public health, biology, and environmental science with specific applications including fMRI, spectroscopic imaging, gene microarrays among many others, presenting a need for effective covariance estimation in the challenging situation where parameter dimensionality is possibly much larger than the number of observations. Additional difficulty due to constraints required to yield positive definite estimates make covariance estimation a potentially complex optimization problem. Further, most existing approaches to covariance estimation require data to be sampled at regular grid (time) points, with subjects sharing a set of common observation points. However, in many practical situations, data are irregularly sampled, and subjects may share few common observation times, and methods are needed to accommodate for data collected in this way.  

To address the challenge of enforcing positive definiteness, several have considered modeling various matrix decompositions including variance-correlation decomposition, spectral decomposition, and Cholesky decomposition. The Cholesky decomposition has received particular attention, as it which allows for a statistically meaningful interpretation as well as an unconstrained parameterization of elements of the covariance matrix. This parameterization allows for estimation to be accomplished as simply as in least squares regression. 

[ ADD citation for the first to introduce using the modified cholesky decomposition ]

It is well known that the sample covariance matrix is unstable in high dimensions, and there is an extensive existing body of work addressing the issue of high dimensionality in the context of covariance estimation. [cite the pourahmadi survey paper here.] However, much of this work addresses high dimensionality arising from functional or times series data sampled on a dense, regular grid. With such data, it is typical that the number of time points is larger than the number of observations. Few have addressed the challenges posed by sparse longitudinal data where measurement times may be almost unique yet sparsely distributed within the observed time range for each individual in the study. In this case, high dimensionality may not be a consequence of having more measurements per subject than the number of subjects themselves, but rather because when pooled across subjects, the total number of unique observed time points is greater than the number of individuals. Incomplete and unbalanced data arise when measurement schedules with targeted time points which are not necessarily equally spaced or if there is missing data. Sparse longitudinal data arise when the measurement schedule has arbitrary or almost unique time points for every individual. A given time point may have very few individuals with corresponding measurements. 

We sidestep both issues of high dimensionality and irregularly sampled data by viewing the response as a stochastic process having continuous covariance function. Recent work outlines the use of function estimation for smoothing elements of the covariance matrix, including [cite all the smoothing papers here. ] To our knowledge, however, no previous work has applied bivariate smoothing both dimensions of the Cholesky factor. We model the generalized autoregressive parameters using tensor product splines. Viewing covariance modeling as bivariate function estimation both accommodates irregularly sampled curved and permits interpolation and extrapolation of the covariance function between two measurements at any pair of time points within the time interval of interest rather than at observed pairs of time points only. Through the Cholesky decomposition, we carry out estimation by the estimation of varying coefficient model. A transformation of the design point axes allows for an ANOVA-like decomposition of the coefficient function into two components, corresponding to the lag between time points and an additive component. Through this general framework, we can easily impose penalties on fitted functions to yield natural null models presented in the literature. 


\section{Cholesky Decomposition of $\Sigma$}

To present a comprehensive overview our estimation procedure, we begin with the representation of the inverse covariance matrix, $\Sigma^{-1}$, in terms of its Cholesky decomposition (see citet{pourahmadi2007cholesky} for a detailed discussion.) In the section to follow, we will demonstrate that this parameterization of the precision matrix is particularly attractive due to both the computational advantages as well as the convenient modeling interpretation it permits. For any positive definite matrix $\Sigma$, there exists a unique unit lower triangular matrix $T$ with diagonal entries equal to $1$ which diagonalizes $\Sigma$:

\begin{equation}
\nonumber T \Sigma T^T = D
\end{equation}
\noindent

If we assume that the data having covariance matrix $\Sigma$ follow an autoregressive model, then the entries of the Cholesky factor $T$ and $D$ enjoy a useful interpretation. Let $Y = \left( y_{1}, y_{2}, \dots, y_{m} \right)^T$ denote a mean-zero random vector of observations with corresponding measurement times 

\[
t_{1} < t_{2} < \dots< t_{m}.
\]

Consider regressing $y_{j}$  on its predecessors:

\begin{equation}
{y}_{j}  = \sum_{k=1}^{j-1} \phi_{jk} y_{k} + \sigma_{j}e_{j}, \qquad j=2,\dots,m, \label{eq:ARmodel}
\end{equation}
\noindent where we define $y_{1}=e_{1}$. Standard regression theory gives us that if $\lbrace \phi_{jk} \rbrace$ are the coefficients of the linear least squares predictor of $y_{j}$ based on its predecessors, then the residuals $e =\left( e_{1}, e_{2},\dots, e_{m} \right)^T$ have diagonal covariance. Let $T$ denote the $m \times m$  matrix with elements 

\[
T_{jk} = \left\{
\begin{array}{ll}
-\phi_{jk} & j > k\\
1 & j = k \\
0 & otherwise,
\end{array}\right.
\]
\noindent
for $j,k=1,\dots,m$. Then in matrix notation,  model~\ref{eq:ARmodel} may then be written

\begin{equation}
e = T Y, \label{eq:epsilon}
\end{equation}
\noindent

Taking covariances on both sides of \ref{eq:epsilon}, we have

\begin{equation}
\nonumber
D = T \Sigma T^T
\end{equation} 

The regression coefficients $\lbrace \phi_{jk} \rbrace$, which are unconstrained, are referred to as the \emph{generalized autoregressive parameters} (GARPs). The $\lbrace \sigma_{j} \rbrace$ are called the \emph{innovation variances} (IVs.)  Unconstrained estimation of the $\left\{ \sigma_{k}^2 \right\}$ is achieved by log transformation;  we leave these details for section 2.) Expressing the precision matrix  in terms of the GARPs and IVs, we have
\begin{equation} \label{eq:omega_decomp}
\Omega= \Sigma^{-1} = T^T D^{-1} T.
\end{equation}

To extend this estimation framework to accommodate observations on multiple subjects which may be taken at unequally spaced and individual-specific observation times. Rather than $m$-dimensional vectors, consider $Y$ and $e$ as the values of the stochastic processes $Y\left(t\right)$ and $e\left(t\right)$ at the set of observation times.  We assume that $Y\left(t\right)$ is equipped with covariance function $G\left(s,t\right)$, and
\[
e\left(s\right) \sim \mathcal{WN}\left(0,1\right)
\] 
is a zero mean Gaussian white noise process with unit variance. For a well-behaved process $Y$, we may assume that $G\left(s,t\right)$ satisfies some smoothness conditions, where smoothness is defined in terms of square integrability of certain derivatives. The entries of $\Sigma$, then, correspond to $G$ evaluated at the distinct pairs of observed time points. Similarly, we treat the elements of the precision matrix $\Omega$ as the values of some smooth function, $\omega\left(s,t\right)$ evaluated at observed pairs of time points. Extention of this perspective to the elements of $D$ and the elements of the Cholesky factor $T$ leads us to  %view the GARPs $\lbrace \phi_{ijk} \rbrace$ and innovation variances as the evaluation of the smooth functions $\phi\left(s,t\right)$ and $\sigma^2\left(t\right)$ at observed time points and interpret $\phi_{ijk} = \phi\left(t_{ij},t_{ik}\right)$ and $\sigma_{ij}^2 = \sigma^2\left(t_{ij}\right)$. 
the varying coefficient (VC) models first introduced by Hastie and Tibshirani. A generalization of traditional linear regression models, varying coefficient models offer more flexibility than their static analogues by allowing the effect of covariates to change smoothly with the value other variables. Both regressors and response variables are assumed to vary according to an \emph{indexing variable}, which is particularly attractive because this permits interpolation of regressors and response variables at values of this indexing variable where there is either missing data of only a single observation and slope estimation is not feasible. Replacing  $\left \{ \phi_{jk} \right\}$ and $\left\{ \sigma_j \right\}$ with smooth functions, we model 

\begin{equation}   
y\left(t_j \right)  = \sum_{k=1}^{j-1} \phi\left(t_j ,t_k\right) y\left(t_k\right) + \sigma\left(t_j\right)\epsilon\left({t_j}\right) \;\;\;\; j=1,\dots, m, 
\label{eq:MyModel} 
\end{equation}

\noindent for $t_1 < t_2 < \dots < t_m$. 
Within our proposed framework, the task of estimating a covariance matrix is equivalent to estimating the function $\phi\left(s,t\right)$. We explore using both smoothing splines and B-spline expansions for function approximation; to induce smoothness and parsimony of estimated Cholesky For ease of exposition, we first assume that $\sigma^2\left(t\right)$ is fixed and known; we will later relax this assumption and discuss the estimation of $\sigma^2$ and $\phi$ simultaneously.  We assume nothing about the functional form of $\phi$ other than that $\phi$ is smooth, with smoothness, which is defined in terms of families of penalties which we will discuss in detail in sections to follow. %square integrability of certain derivatives. 
Our approach presents a flexible, comprehensive framework for covariance estimation in a wide variety of contexts in which the data may be generated according to a broad class of dependency structures.
%Observed time points may be individual-specific and not necessarily on a regular grid. 

\section{Penalized Maximum Likelihood Estimation of $\phi$}

Along with citet{huang2006covariance}, citet{levina2008sparse}, and citet{pourahmadi2000maximum} we employ the Gaussian log-likelihood as a goodness of fit measure for the varying  autoregressive coefficient function, $\phi\left(t,s\right)$ and the innovation variance function $\sigma\left(t\right)$, though neither the derivation of model~\ref{eq:ARmodel} nor model~\ref{eq:MyModel} rely on any assumptions about the distribution of $e$. Fixing $\sigma_j^2$, the negative loglikelihood as a function of $\phi_{jk}$ corresponds to the usual error sums of squares.  Under the gaussian assumption, for fixed $\left\{ \sigma_j^2 \right\}$, the negative log-likelihood of $N$ i.i.d. vectors of observations $y_1,y_2,\dots,y_N$ is proportional to

\begin{equation}
-2 L\left(y_1, y_2, \dots,y_N ,\Phi \right) \propto \sum_{i=1}^N \sum_{j=2}^{m_i} \sigma\left({t_j}\right)^{-2} \left(y_{ij} - \sum_{k=1}^{j-1}\phi\left({t_{ij},t_{ik}}\right)y_{ik} \right)^2 \label{loglikelihood}
\end{equation}
\noindent
where 
\[
y_i = \left( y_{i1}, y_{i2}, \dots, y_{i,m_i}\right), \quad i=1,\dots,N 
\] 

%% explain here that we omit y_{i1} from the likelihood here because in the first stage of estimation (for $\phi$), the first observation in each vector doesn't contribute to the likelihood
\noindent
denotes the vector of observations for subject $i$ with corresponding measurement times 
\[
t_{i1} < t_{i2} < \dots < t_{i,m_i}.
\]
The form of the likelihood of $y_1,\dots,y_N$ indicates that we allow both the number of measurements as well as the observation times to varying across subjects. The $\left\{t_{ij} \right\}$ need not be evenly-spaced within or across individuals.  In the case that subjects share a common set of observation times $t_1 < \dots < t_m$,  it is well known that the MLE for $\Sigma$, $S = \sum_{i=1}^N y_i y_i^T$ is highly unstable in high dimensions. [cite those who have proposed mitigating this using penalized maximum likelihood for the distinct elements of T.] This condition is potentially worsened when one or more subjects has at least one unique observation time. To mitigate instability due to high dimensionality and simultaneously permit the estimation of $\phi\left(\cdot,\cdot\right)$ as a smooth bivariate function, we obtain a covariance estimator by applying bivariate smoothing of the elements of the Cholesky factor. We impose two families of penalties on fitted functions, leading to two distinct parameterizations of the smoothed function [blah blah something about flexibility and penalty being problem-specific.]  


\section{Representation of $\phi$ as a smooth function}

To impose structure on the estimated autoregressive function, we append a penalty functional to the negative log-likelihood \ref{eq:}. that discourages the flexibility of the fitted function. We take the estimator of $\phi$ to minimize

%\begin{itemize}
%\item {\bf Step 1:} Select $\hat{\lambda}_1$, where
\begin{equation} 
 -2 L + \lambda J_{\phi}\left(\phi\right)  \label{eq:phi-objective-fnc}.
\end{equation}
%\item {\bf Step 2:} Define $\hat{\phi}$ to the the minimizer of 


The task of estimating of $\phi\left(t,s\right)$ is an inherently different problem than the estimation of an arbitrary bivariate function. Since $\phi$ explicitly defines an inverse covariance function, there is more interest in imposing particular kinds of structure on $\phi$ than others. It is natural to define covariance models for longitudinal or time series data in terms of lag, or in the continuous case, the difference between two measurement times. By transforming the original $t-s$ axis according to  
\begin{align*}
l &= s-t, \mbox{ and} \\
m &= \frac{1}{2}\left(s+t\right),
\end{align*}
\noindent
we may parameterize the coefficient function in terms of functional component that are naturally of more interest than the corresponding components in the original input space. Writing $phi$ in terms of the rotation gives the reparameterized coefficient function 
\begin{equation}
\phi^*\left(l,m\right) = \phi^*\left(s-t, \frac{1}{2}\left(s+t\right)\right) = \phi\left(s,t\right).
\end{equation}

We define our estimator $\hat{\phi^*}$ as the minimizer of

\begin{equation} 
-2 L + \lambda^* J_{\phi^*}\left(\phi^*\right) \label{eq:phi-star-objective-fnc}.
\end{equation}

\subsection{Smoothing spline ANOVA models}

We consider models that capture the marginal effects of $l$ and $m$, as well as interaction between the two directions. We first consider the smoothing spline ANOVA decomposition of citet{gu2002smoothing},  modeling 

\begin{equation}
\phi^*\left(l,m\right) = \mu^* + \phi_1^*\left(l\right) + \phi_2^*\left(m\right) + \phi_{12}^*\left(l,m\right).   \label{eq:ANOVA}
\end{equation}
\noindent 

As in cite{gu2002smoothing}, cite{craven1978smoothing},[ more Wahba citations here ], we consider functions $\phi^*$ belonging to a reproducing kernel Hilbert space (r.k.h.s.), $\mathcal{H}$. The space of bivariate functions can be constructed from the tensor product of the univariate function spaces for $l$ and $m$. We equip $l$ and $m$ each with r.k.h.s., $\mathcal{H}_l$ and $\mathcal{H}_m$. Then the space of bivariate functions $\mathcal{H}$ may be written:

\[
\mathcal{H} = \mathcal{H}_l \otimes \mathcal{H}_m.
\]
\noindent
One choice is to let the marginal spaces correspond to the second-order Sobolev space: $\mathcal{H}_l = \mathcal{H}_m = W_2\left(0,1\right)$, where
\[
W_2\left(0,1\right) = \lbrace f: \;\;f, f^\prime \mbox{absolutely continuous}, \int_0^1 \left(f^{\left( 2 \right)}\right)^2 dt < \infty \rbrace.
\]  

\subsection{Decomposition of $\mathcal{H}$ via $J_{\phi^*}$}

Several approaches have been taken in effort to overcome the issue of high dimensionality in covariance estimation'  citet{pourahmadi2011covariance} provides a comprehensive overview of covariance estimation from a generalized linear modeling perspective. Several have proposed methods for applying regularization of Cholesky decomposition including banding, tapering, kernel smoothing, penalized likelihood, and penalized regression. See [  ] Within the function estimation paradigm, a number of  approaches to estimate the coefficient function $\phi\left(\cdot,\cdot\right)$ have been proposed including See cite{wu2003nonparametric},  cite{huang2007estimation} .  Common techniques for inducing structure to produce simple and stable covariance estimates include shrinking estimated functions or the elements of the covariance matrix itself so that the resulting dependency structure corresponds to parsimonious covariance models frequently adopted in the time series and longitudinal  data literature.
[
CITE PAPERS PROPOSING PARSIMONIOUS MODELS FOR phi ij
]
The ANOVA model in \ref{eq:ANOVA} allows us to easily specify penalties $J$ that encourage estimates to adhere to the structure of these models.   [cite some general time series/longitudinal sources ] When $\phi^*$ corresponds to the simple models of the form \eqref{covmodel}, the bivariate function may be written in terms of only its first argument. 
. . %cite{wu2003nonparametric}, for example, used locally weighted polynomials to smooth down the sub-diagonals of $T$. cite{huang2007estimation} smoothed the sub-diagonals of $T$ using univariate smoothing splines. 


The penalty functional penalty functional $J$ induces a decomposition of $\mathcal{H}$ into a direct sum of two function spaces: the null space of $J$ composed of functions incurring no penalty, and the orthogonal space of penalized functions. 

\[
\mathcal{H} = \mathcal{H}_0 \oplus \mathcal{H}_1
\]
\noindent

Let $P_1 \phi^*$  denote the projection of $\phi^*$ onto the penalized space $\mathcal{H}_1$. We can express $J$ in terms of the projection of $\phi^* \in \mathcal{H}$ onto $\mathcal{H}_1$:
\begin{align}
\begin{split} 
J\left(\phi\right) &= \vert \vert P_1 \phi^* \vert \vert^2\\
&= \vert \vert {P_1 \phi_1^*} \vert \vert^2 + \vert \vert {P_1 \phi_2^*} \vert \vert^2 + \vert \vert {P_1 \phi_{12}^*} \vert \vert^2  \label{eq:SS_penalty}
\end{split}
\end{align} 
\noindent

$P_1 \phi^*$ denotes the projection of $\phi^* \in \mathcal{H}$ onto $\mathcal{H}_1$.The penalty functional $J_1$ induces a decomposition of $\mathcal{H}$ as follows: $\mathcal{H}_l = \mathcal{H}_l^0 \oplus \mathcal{H}_l^1$ and $\mathcal{H}_m = \mathcal{H}_m^0 \oplus \mathcal{H}_m^1$ where let $\mathcal{H}_l^0 =  \lbrace 1 \rbrace \oplus \lbrace k_1 \rbrace$, $\mathcal{H}_m^0 =  \lbrace  1 \rbrace$, and where $\lbrace k_r \rbrace$ denotes the subspace spanned by $k_r$. $\mathcal{H}_l^1$ and $\mathcal{H}_m^1$ are the subspaces orthogonal to $\mathcal{H}_l^0$ and $\mathcal{H}_m^0$, respectively:
\begin{eqnarray*}
\mathcal{H}_l^1 = \lbrace \phi^*_1: \int_0^1 {\phi_1^*}^{\left( \nu \right)}\left(l\right) dl = 0,\;\; \nu = 0,1\rbrace\\
\mathcal{H}_m^1 = \lbrace \phi^*_2: \int_0^1 \phi_2^* \left(m\right) dm = 0 \rbrace\\
\end{eqnarray*}
\noindent
Using the properties of tensor product spaces, we may decompose $\mathcal{H} = \mathcal{H}^0\oplus \mathcal{H}^1$ where
\begin{eqnarray*}
\mathcal{H}^0 &=& \lbrace 1 \rbrace \oplus \lbrace k_1 \rbrace\\
\mathcal{H}^1 &=& \mathcal{H}_l^1 \oplus \mathcal{H}_m^1 \oplus  \left[ \lbrace k_1 \rbrace  \otimes  \mathcal{H}_m^1 \right]  \oplus  \left[\mathcal{H}_l^1 \otimes  \mathcal{H}_m^1\right]   
\end{eqnarray*}

 To find the solution $\hat{\phi^*}$ which is the stage-wise minimizer of \eqref{objfun2}: we first set $\lambda_2 = 0$ and find $\tilde{\phi}^*$ which minimizes \eqref{objfun1}:

\begin{equation}
-2L + \lambda_1 J_1\left(\phi^*\right) = \sum_{i=1}^N \sum_{j=2}^{p_i} \sigma\left({t_j}\right)^{-2} \left(y\left({t_{ij}}\right) - \sum_{k=1}^{j-1}\phi\left({t_{ij},t_{ik}}\right)y\right)^2 + \lambda_1 \left(\vert \vert {P_1 \phi_1^*} \vert \vert^2 + \vert \vert {P_1 \phi_2^*} \vert \vert^2 + \vert \vert {P_1 \phi_{12}^*} \vert \vert^2 \right) \label{stage1obj}
\end{equation}



\subsection{Outline Smoothing Spline Approach}

\subsection{The truncated power basis and an alternative decomposition of $\mathcal{H}$}

The estimation of $\phi^*\left(l,m\right)$ is quite different from the usual problem of estimating an arbitrary bivariate function via smoothing. In the case of the latter, we most typically treat both arguments equally in terms of regularization, but in the case of covariance estimation and the generalized coefficient function equal treatment of $l$ and $m$ in terms of penalization perhaps is not the most appropriate approach. The lag component, $l$, has particularly significant meaning in terms of the covariance function and thus also in terms of $\phi^*$ and is of considerable more interest than the orthogonal component, $m$. As discussed in Section 2, we can define an entire class of stationary functional autoregressive models using only the $l$ direction, and additionally, as discussed in Section 3, there is a natural expectation about the functional form of the autoregressive coefficient function (and hence covariance) as a function of $l$, making imposing that conditional dependence between observation decay as $l$ and the time between observations increase a reasonable way to institute regularization.
%citet{chen2011efficient}, citet{pourahmadi1999joint}, and citet{pourahmadi2002dynamic} have elicited parametric models for the generalized autoregressive coefficients, letting the GARPs depend only on the distance between two time points.

This latter notion is instrumental in justifying the family of penalties
\[
J_{2,\left(p\right)} = \sum_{  l_i \in \mathcal{L}: l_i > l_0} \vert \mu^* + \phi^*_1\left(l_i\right) \vert^p 
\]
\noindent
which we may view as a design-driven way of implementing the regularization which may be imposed by the penalty functionals taking the form

\begin{eqnarray} \nonumber
J\left(\phi^*\right) &=& \int_{l_0}^1 \vert \mu^* + \phi^*_1\left(l\right) \vert^p\; dl\\
&=& \int_{0}^1 \vert \mu^* + \phi^*_1\left(l\right) \vert^p I\left(l > l_0\right) \; dl \label{eq:truncated_penalty}
\end{eqnarray}

The penalty functionals given by \eqref{truncated_penalty} motivate a different decomposition of $\mathcal{H}$ than the derivative-based penalty. The form of \eqref{truncated_penalty} is significantly different in nature from the penalty discussed in Section 2.1 and those typically encountered in the setting smoothing spline ANOVA models, particularly because \eqref{truncated_penalty} effects only a subset of the domain for $l$. Therefore, an appropriate decomposition of the function space into the null space of $J$ and the penalized space should perhaps be formulated in terms of basis functions for the lag component, $l$ with domains which do not include the entire unit interval. 

The truncated power basis, as in their use in defining polynomial regression splines, enjoy a particular ease of interpretation, as the coefficient $\beta_{i+k}$ may be identified as the size of the jump at $x_i$ in the $k^{th}$ derivative of $f$. This fact is especially useful when tracking change points or, in general, any abrupt changes in the regression curve. If we reflect these basis functions about each of their corresponding knot points and denote these reflections $\lbrace T^-_{ik}\rbrace$, then expressing the regularization corresponding to the penalty functionals \eqref{truncated_penalty} becomes quite natural in terms of the reflected basis functions $\left(\cdot - l_1 \right)^k_-,\dots, \left(\cdot - l_n \right)^k_-$, where $\left( \alpha \right)_- = \max\left(-\alpha,0\right)$.  While the truncated power basis initially appears very attractive for representing functions in terms of the decomposition induced by penalties of the same form as that in Equation~\ref{eq:truncated_penalty}, they 

\section{P-splines}

\subsection{Truncated Power Basis}

\subsection{B-spline Basis}
\subsection{Difference penalties}

\end{document}
