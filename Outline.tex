\documentclass[12pt]{article}
\usepackage{graphicx,psfrag,amsfonts,float,mathbbol,xcolor,cleveref}
\usepackage{arydshln}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage[mathscr]{euscript}
\usepackage{enumitem}
\usepackage{accents}
\usepackage{framed}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{mathtools}
\usepackage{IEEEtrantools}
\usepackage{times}
\usepackage{cite}
\usepackage{amsthm}
\usepackage[letterpaper, left=1in, top=1in, right=1in, bottom=1in,nohead,includefoot, verbose, ignoremp]{geometry}
\newcommand{\comment}[1]{\text{\phantom{(#1)}} \tag{#1}}
\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand*\needsparaphrased{\color{red}}
\newcommand*\needscited{\color{orange}}
\newcommand*\needsproof{\color{blue}}
\newcommand*\outlineskeleton{\color{green}}
\newcommand{\PP}{\mathcal{P}}
\newcommand{\bfeps}{\mbox{\boldmath $\epsilon$}}
\newcommand{\bfgamma}{\mbox{\boldmath $\gamma$}}
\newcommand{\bflam}{\mbox{\boldmath $\lambda$}}
\newcommand{\bfphi}{\mbox{\boldmath $\phi$}}
\newcommand{\bfsigma}{\mbox{\boldmath $\sigma$}}
\newcommand{\bfbeta}{\mbox{\boldmath $\beta$}}
\newcommand{\bfalpha}{\mbox{\boldmath $\alpha$}}
\newcommand{\bfe}{\mbox{\boldmath $e$}}
\newcommand{\bff}{\mbox{\boldmath $f$}}
\newcommand{\bfone}{\mbox{\boldmath $1$}}
\newcommand{\bft}{\mbox{\boldmath $t$}}
\newcommand{\bfo}{\mbox{\boldmath $0$}}
\newcommand{\bfO}{\mbox{\boldmath $O$}}
\newcommand{\bfx}{\mbox{\boldmath $x$}}
\newcommand{\bfX}{\mbox{\boldmath $X$}}
\newcommand{\bfz}{\mbox{\boldmath $z$}}


\newcommand{\bfm}{\mbox{\boldmath $m}}
\newcommand{\bfy}{\mbox{\boldmath $y$}}
\newcommand{\bfa}{\mbox{\boldmath $a$}}
\newcommand{\bfb}{\mbox{\boldmath $b$}}
\newcommand{\bfY}{\mbox{\boldmath $Y$}}
\newcommand{\bfS}{\mbox{\boldmath $S$}}
\newcommand{\bfZ}{\mbox{\boldmath $Z$}}
\newcommand{\cardT}{\vert \mathcal{T} \vert}
%\newenvironment{theorem}[1][Theorem]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{corollary}[1][Corollary]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{proposition}[1][Proposition]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}
%\newenvironment{definition}[1][Definition]{\begin{trivlist}
%\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{example}{Example}[section]
\def\bL{\mathbf{L}}


\makeatletter
\renewcommand{\theenumi}{\Roman{enumi}}
\renewcommand{\labelenumi}{\theenumi.}
\renewcommand{\theenumii}{\Alph{enumii}}
\renewcommand{\labelenumii}{\theenumii.}
\renewcommand{\p@enumii}{\theenumi.}
\makeatother

\begin{document}

%\nocite{*}
\def\bL{\mathbf{L}}
%\usepackage{mathtime}

%%UNCOMMENT following line if you have package


\title{ Nonparametric Covariance Estimation for Longitudinal Data via Penalized Tensor Product Splines}

\author{Tayler A. Blake\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201} \and  Yoonkyung Lee\thanks{The Ohio State University, 1958 Neil Avenue, Columbus, OH 43201}}

\bibliographystyle{plainnat}
\maketitle

\begin{abstract}
With high dimensional longitudinal and functional data becoming much more common, there is a strong need for methods of estimating large covariance matrices. Estimation is made difficult  by the instability of sample covariance matrices in high dimensions and a positive-definite constraint we desire to impose on estimates. A Cholesky decomposition of the covariance matrix allows for parameter estimation via unconstrained optimization as well as a statistically meaningful interpretation of the parameter estimates. Regularization improves stability of covariance estimates in high dimensions, as well as in the case where functional data are sparse and individual curves are sampled at different and possibly unequally spaced time points. By viewing the entries of the covariance matrix as the evaluation of a continuous bivariate function at the pairs of observed time points, we treat covariance estimation as bivariate smoothing. 

Within regularization framework, we propose novel covariance penalties which are designed to yield natural null models presented in the literature for stationarity or short-term dependence. These penalties are expressed in terms of variation in continuous time lag and its orthogonal complement. We present numerical results and data analysis to illustrate the utility of the proposed method. \\
\\
%\begin{keywords}
{\bf keywords:} non-parametric, covariance, longitudinal data, functional data, splines, reproducing kernel Hilbert space
%\end{keywords}
\end{abstract}

\section{Introduction}

An estimate of the covariance matrix or its inverse is required for nearly all statistical procedures in classical multivariate data analysis, time series analysis, spatial statistics and, more recently, the growing field of statistical learning. Covariance estimates play a critical role in the performance of techniques for clustering and classification such as linear discriminant analysis (LDA), quadratic discriminant analysis (QDA), factor analysis, and principal components analysis (PCA), analysis of conditional independence through graphical models, classical multivariate regression, prediction, and Kriging. Covariance estimation with high dimensional data has has recently gained growing interest; it is generally recognized that there are two primary hurdles responsible for the difficulty in covariance estimation: the instability of sample covariance matrices in high dimensions and a positive-definite constraint we wish estimates to obey.

Prevalent technological advances in industry and many areas of science make high dimensional longitudinal and functional data a common occurrence, arising in numerous areas including medicine, public health, biology, and environmental science with specific applications including fMRI, spectroscopic imaging, gene microarrays among many others, presenting a need for effective covariance estimation in the challenging situation where parameter dimensionality $p$ is possibly much larger than the number of observations, $n$. Additional difficulty due to constraints required to yield positive definite estimates make covariance estimation a potentially complex optimization problem. Further, most existing approaches to covariance estimation require data to be sampled at regular grid (time) points, with subjects sharing a set of common observation points. However, in many practical situations, data are irregularly sampled, and subjects may share few common observation times, and methods are needed to accommodate for data collected in this way.  

To address the challenge of enforcing positive definiteness, several have considered modeling various matrix decompositions including variance-correlation decomposition, spectral decomposition, and Cholesky decomposition. The Cholesky decomposition has received particular attention, as it which allows for a statistically meaningful interpretation as well as an unconstrained parameterization of elements of the covariance matrix. This parameterization allows for estimation to be accomplished as simply as in least squares regression. 

[ ADD citation for the first to introduce using the modified cholesky decomposition ]

It is well known that the sample covariance matrix is unstable in high dimensions. [add citation.]  There is an extensive body of work addressing the issue of high dimensionality in the context of covariance estimation. [cite the pourahmadi survey paper here.] However, much of this work address high dimensionality arising from functional or times series data sampled on a dense, regular grid. With such data, it is typical that the number of time points is larger than the number of observations. Few have addressed the challenges posed by sparse longitudinal data where measurement times may be almost unique yet sparsely distributed within the observed time range for each individual in the study. In this case, high dimensionality may not be a consequence of having more measurements per subject than the number of subjects themselves, but rather because when pooled across subjects, the total number of unique observed time points is greater than the number of individuals. Incomplete and unbalanced data arise when measurement schedules with targeted time points which are not necessarily equally spaced or if there is missing data. Sparse longitudinal data arise when the measurement schedule has arbitrary or almost unique time points for every individual. A given time point may have very few individuals with corresponding measurements. 

We sidestep both issues of high dimensionality and irregularly sampled data by viewing the response as a stochastic process having continuous covariance function. We model the generalized autoregressive parameters as the evaluation of a continuous bivariate function at the pairs of observed time points. This allows us to accommodate problematic sampling as well as estimate the covariance between two measurements at any pair of time points within the time interval of interest, and not just at the observed pairs of time points. Through the Cholesky decomposition, we carry out estimation by the estimation of varying coefficient model. A transformation of the design point axes allows for an ANOVA-like decomposition of the coefficient function into two components, corresponding to the lag between time points and an additive component. Through this general framework, we can easily impose penalties on fitted functions to yield natural null models presented in the literature. 


\section{Cholesky Decomposition of $\Sigma$}

To present a comprehensive overview our estimation procedure, we begin with the representation of the inverse covariance matrix, $\Sigma^{-1}$, in terms of its Cholesky decomposition (see citet{pourahmadi2007cholesky} for a detailed discussion). Decomposing the precision matrix in such a way allows for both an unconstrained parameterization and statistically meaningful interpretation of covariance parameters. For any positive definite matrix $\Sigma$, there exists a unique unit lower triangular matrix $T$ with diagonal entries equal to $1$ which diagonalizes $\Sigma$:

\begin{equation}
\nonumber T \Sigma T^T = D
\end{equation}
\noindent

Let $Y = \left( y_{1}, y_{2}, \dots, y_{m} \right)^T$ denote a mean-zero random vector of observations with corresponding measurement times 

\[
t_{1} < t_{2} < \dots< t_{m}.
\]

If we assume that the data follow an autoregressive model, then the entries of the Cholesky factor $T$ and $D$ enjoy a useful interpretation. Regress $y_{j}$ on its predecessors:

\begin{equation}
{y}_{j}  = \sum_{k=1}^{j-1} \phi_{jk} y_{k} + \sigma_{j}e_{j}, \qquad j=2,\dots,m, \label{eq:data_ARmodel}
\end{equation}
\noindent where we define $y_{1}=e_{1}$. Standard regression theory gives us that if $\lbrace \phi_{jk} \rbrace$ are the coefficients of the linear least squares predictor of $y_{j}$ based on its predecessors, then the residuals $e =\left( e_{1}, e_{2},\dots, e_{m} \right)^T$ have diagonal covariance. Let $T$ be the unit lower triangular matrix with elements $T_{jk}$, $j,k=1,\dots,m$, where

\[
T_{jk} = \left\{
\begin{array}{ll}
-\phi_{jk} & j > k\\
1 & j = k \\
0 & otherwise.
\end{array}\right.
\]

Then model~\ref{eq:ARmodel} as follows: 

\begin{equation}
e = T Y, \label{epsilon}
\end{equation}
\noindent
Taking covariances on both sides of \eqref{epsilon}, we have

\begin{equation}
\nonumber
D = T \Sigma T^T
\end{equation} 

The regression coefficients $\lbrace \phi_{jk} \rbrace$ are referred to as the \emph{generalized autoregressive parameters} (or GARPs), and the $\lbrace \sigma_{j} \rbrace$ are referred to as the \emph{innovation variances} (or IVs.) It is worth noting that  particularly attractive property of decomposing $\Omega$ in this way is that the $\left\{ \phi_{jk} \right\}$ are not constrained. (Estimation of the  $\left\{ \sigma_{k}^2 \right\}$ is also unconstrained, but we leave the details for section 2.) Immediately, we can express the precision matrix (and thus its inverse) entirely in terms of these parameters:
\begin{equation} \label{eq:omega_decomp}
\Omega= \Sigma^{-1} = T^T D{-1} T.
\end{equation}

We extend this framework to accommodate unequally spaced and subject-specific observation times and consider $Y$ and $e$ as vectorized the stochastic processes: $Y\left(t\right)$ and $e\left(t\right)$.  We assume $Y\left(t\right)$ has covariance function $G\left(s,t\right)$, and
\[
e\left(s\right) \sim \mathcal{WN}\left(0,1\right)
\] 
is a zero mean Gaussian white noise process with unit variance. For a well-behaved process $Y$, we may assume that $G\left(s,t\right)$ satisfies some smoothness conditions, where smoothness is defined in terms of square integrability of certain derivatives. The entries of $\Sigma$ then correspond to $G$ evaluated at the distinct pairs of observed time points, and similarly, we treat the elements of the precision matrix $\Omega$ as the values of some smooth function, $\omega\left(s,t\right)$ evaluated at observed time points. A natural extention of the same perspective to the elements of $D$ as well as the Cholesky factor $T$ leads us to  %view the GARPs $\lbrace \phi_{ijk} \rbrace$ and innovation variances as the evaluation of the smooth functions $\phi\left(s,t\right)$ and $\sigma^2\left(t\right)$ at observed time points and interpret $\phi_{ijk} = \phi\left(t_{ij},t_{ik}\right)$ and $\sigma_{ij}^2 = \sigma^2\left(t_{ij}\right)$. 
the varying coefficient (VC) models first introduced by Hastie and Tibshirani, which assume that  regressors and response variables vary according to an \emph{indexing variable}. These models are particularly attractive because they permit interpolation of regressors and response variables at values of this indexing variable where there is either missing data of only a single observation and slope estimation is not feasible. Replacing  $\left \{ \phi_{jk} \right\}$ and $\left\{ \sigma_j \right\}$ with smooth functions, we model 

\begin{equation}   
y\left(t_j \right)  = \sum_{k=1}^{j-1} \phi\left(t_j ,t_k\right) y\left(t_k\right) + \sigma\left(t_j\right)\epsilon\left({t_j}\right) \;\;\;\; j=1,\dots, m, 
\label{eq:MyModel} 
\end{equation}

\noindent for $t_1 < t_2 < \dots < t_m$. 
Within our proposed framework, the task of estimating a covariance matrix is equivalent to estimating the function $\phi\left(s,t\right)$. We explore using both smoothing splines and B-spline expansions for function approximation; to induce smoothness and parsimony of estimated Cholesky For ease of exposition, we first assume that $\sigma^2\left(t\right)$ is fixed and known; we will later relax this assumption and discuss the estimation of $\sigma^2$ and $\phi$ simultaneously.  We assume nothing about the functional form of $\phi$ other than that $\phi$ is smooth, with smoothness, which is defined in terms of families of penalties which we will discuss in detail in sections to follow. %square integrability of certain derivatives. 
Our approach presents a flexible, comprehensive framework for covariance estimation in a wide variety of contexts in which the data may be generated according to a broad class of dependency structures.
%Observed time points may be individual-specific and not necessarily on a regular grid. 

\section{Penalized Maximum Likelihood Estimation of $\phi$}

Along with citet{huang2006covariance}, citet{levina2008sparse}, and citet{pourahmadi2000maximum} we employ the Gaussian log-likelihood as a goodness of fit measure for the varying  autoregressive coefficient function, $\phi\left(t,s\right)$ and the innovation variance function $\sigma\left(t\right)$, though neither the derivation of model~\ref{eq:ARmodel} nor model~\ref{eq:MyModel} rely on any distributional assumption on $e$.   Under the assumption that $e\left(t\right)$ follows a Gaussian process, the negative log-likelihood of $N$ i.i.d. vectors of observations $y_1,y_2,\dots,y_N$ is given by

\begin{equation}
-2 L\left(y_1, y_2, \dots,y_N ,\Phi \right) = \sum_{i=1}^N \sum_{j=2}^{m_i} \sigma\left({t_j}\right)^{-2} \left(y_{ij} - \sum_{k=1}^{j-1}\phi\left({t_{ij},t_{ik}}\right)y_{ik} \right)^2 \label{loglikelihood}
\end{equation}
\noindent
where 
\[
y_i = \left( y_{i1}, y_{i2}, \dots, y_{i,m_i}\right), \quad i=1,\dots,N 
\] 

%% explain here that we omit y_{i1} from the likelihood here because in the first stage of estimation (for $\phi$), the first observation in each vector doesn't contribute to the likelihood
\noindent
denotes the vector of observations for subject $i$ with corresponding measurement times $t_{i1} < t_{i2} < \dots < t_{i,m_i}$. We allow the number of measurements as well as the observation times to varying across subjects, and the $\left\{t_{ij} \right\}$ are not necessarily evenly-spaced within or across individuals.  In the case that subjects share a common set of observation times $t_1 < \dots < t_n$,  it is well known that the MLE for $\Sigma$, $S = \sum{i=1}^N y_i y_i^T$ is highly unstable in high dimensions. [cite those who have proposed mitigating this using penalized maximum likelihood for the distinct elements of T.] This condition is potentially worsened when one or more subjects has at least one unique observation time. To simultaneously permit the estimation of $\phi\left(\cdot,\cdot\right)$ as a smooth bivariate function and address any ill-conditioning introduced by high dimensionality, we present covariance estimation by applying bivariate smoothing of the elements of the Cholesky factor. We impose two families of penalties on fitted functions, leading to two distinct parameterizations of the smoothed function [blah blah something about flexibility and penalty being problem-specific.]  




\section{Penalties for $\phi^* \in \mathcal{H}$}

To impose structure on the estimated autoregressive coefficient function, we append a penalty functional to the negative log-likelihood and define our estimator of $\phi$ to be the minimizer of 

%\begin{itemize}
%\item {\bf Step 1:} Select $\hat{\lambda}_1$, where
\begin{equation} 
\hat{\phi} = \mathop{\mbox{arg min}}_{\phi} \left( -2 L + \lambda J\left(\phi\right) \right) \label{stage1obj}
\end{equation}
%\item {\bf Step 2:} Define $\hat{\phi}$ to the the minimizer of 



cite{craven1978smoothing}
Rather than imposing structure on $\phi\left(t,s\right)$ in terms of $t$ and $s$, it is convenient to transform the pairs of time points and apply regularization to the coefficient function in the reparameterized space. We rotate the axes of the time points, letting 
\begin{align*}
l &= s-t, \mbox{ and} \\
m &= \frac{1}{2}\left(s+t\right).
\end{align*}
\noindent
Our goal is to estimate the reparameterized function
\begin{equation}
\phi^*\left(l,m\right) = \phi^*\left(s-t, \frac{1}{2}\left(s+t\right)\right) = \phi\left(s,t\right)
\end{equation}

We first follow ,letting $\phi^*$ be an element of a reproducing kernel Hilbert space (r.k.h.s.), $\mathcal{H}$. If we equip$l$ and $m$ each with r.k.h.s. of functions, $\mathcal{H}_l$ and $\otimes \mathcal{H}_m$, then $\mathcal{H}$ may be written as a tensor product of the two marginal spaces:

\[
\mathcal{H} = \mathcal{H}_l \otimes \mathcal{H}_m.
\]

Adopting the smoothing spline ANOVA decomposition presented by citet{gu2002smoothing},  we let 

\begin{equation}
\phi^*\left(l,m\right) = \mu^* + \phi_1^*\left(l\right) + \phi_2^*\left(m\right) + \phi_{12}^*\left(l,m\right)   \label{ANOVA}
\end{equation}
\noindent 

We let $\mathcal{H}_l = \mathcal{H}_m = W_2\left(0,1\right)$ where $W_2$ denotes the second-order Sobolev space:
\[
W_2\left(0,1\right) = \lbrace f: \;\;f, f^\prime \mbox{absolutely continuous}, \int_0^1 \left(f^{\left( 2 \right)}\right)^2 dt < \infty \rbrace
\]  



When $\phi^*$ corresponds to the simple models of the form \eqref{covmodel}, the bivariate function may be written in terms of only its first argument. 
Several approaches to estimate the coefficient function $\phi\left(\cdot,\cdot\right)$ have been proposed including See cite{wu2003nonparametric},  cite{huang2007estimation} , ... . %cite{wu2003nonparametric}, for example, used locally weighted polynomials to smooth down the sub-diagonals of $T$. cite{huang2007estimation} smoothed the sub-diagonals of $T$ using univariate smoothing splines. 

Several approaches have been taken in effort to overcome the issue of high dimensionality in covariance estimation. Regularization improves stability of covariance estimates in high dimensions, particularly in the case where the parameter dimensionality $p$ is much larger than the number of observations $n$.  Regularization of the covariance matrix and its Cholesky decomposition has been explored extensively. Previous work employs a number of approaches including banding, tapering, kernel smoothing, penalized likelihood, and penalized regression; see citet{pourahmadi2011covariance} for a comprehensive overview.


\subsection{Outline Smoothing Spline Approach}

\subsection{The truncated power basis and an alternative decomposition of $\mathcal{H}$}

The estimation of $\phi^*\left(l,m\right)$ is quite different from the usual problem of estimating an arbitrary bivariate function via smoothing. In the case of the latter, we most typically treat both arguments equally in terms of regularization, but in the case of covariance estimation and the generalized coefficient function equal treatment of $l$ and $m$ in terms of penalization perhaps is not the most appropriate approach. The lag component, $l$, has particularly significant meaning in terms of the covariance function and thus also in terms of $\phi^*$ and is of considerable more interest than the orthogonal component, $m$. As discussed in Section 2, we can define an entire class of stationary functional autoregressive models using only the $l$ direction, and additionally, as discussed in Section 3, there is a natural expectation about the functional form of the autoregressive coefficient function (and hence covariance) as a function of $l$, making imposing that conditional dependence between observation decay as $l$ and the time between observations increase a reasonable way to institute regularization.
%citet{chen2011efficient}, citet{pourahmadi1999joint}, and citet{pourahmadi2002dynamic} have elicited parametric models for the generalized autoregressive coefficients, letting the GARPs depend only on the distance between two time points.

This latter notion is instrumental in justifying the family of penalties
\[
J_{2,\left(p\right)} = \sum_{  l_i \in \mathcal{L}: l_i > l_0} \vert \mu^* + \phi^*_1\left(l_i\right) \vert^p 
\]
\noindent
which we may view as a design-driven way of implementing the regularization which may be imposed by the penalty functionals taking the form

\begin{eqnarray} \nonumber
J\left(\phi^*\right) &=& \int_{l_0}^1 \vert \mu^* + \phi^*_1\left(l\right) \vert^p\; dl\\
&=& \int_{0}^1 \vert \mu^* + \phi^*_1\left(l\right) \vert^p I\left(l > l_0\right) \; dl \label{eq:truncated_penalty}
\end{eqnarray}

The penalty functionals given by \eqref{truncated_penalty} motivate a different decomposition of $\mathcal{H}$ than the derivative-based penalty. The form of \eqref{truncated_penalty} is significantly different in nature from the penalty discussed in Section 2.1 and those typically encountered in the setting smoothing spline ANOVA models, particularly because \eqref{truncated_penalty} effects only a subset of the domain for $l$. Therefore, an appropriate decomposition of the function space into the null space of $J$ and the penalized space should perhaps be formulated in terms of basis functions for the lag component, $l$ with domains which do not include the entire unit interval. 

The truncated power basis, as in their use in defining polynomial regression splines, enjoy a particular ease of interpretation, as the coefficient $\beta_{i+k}$ may be identified as the size of the jump at $x_i$ in the $k^{th}$ derivative of $f$. This fact is especially useful when tracking change points or, in general, any abrupt changes in the regression curve. If we reflect these basis functions about each of their corresponding knot points and denote these reflections $\lbrace T^-_{ik}\rbrace$, then expressing the regularization corresponding to the penalty functionals \eqref{truncated_penalty} becomes quite natural in terms of the reflected basis functions $\left(\cdot - l_1 \right)^k_-,\dots, \left(\cdot - l_n \right)^k_-$, where $\left( \alpha \right)_- = \max\left(-\alpha,0\right)$.  While the truncated power basis initially appears very attractive for representing functions in terms of the decomposition induced by penalties of the same form as that in Equation~\ref{eq:truncated_penalty}, they 

\section{P-splines}

\subsection{Truncated Power Basis}

\subsection{B-spline Basis}
\subsection{Difference penalties}

\end{document}
