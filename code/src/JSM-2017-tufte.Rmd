---
title: "Nonparametric covariance estimation for longitudinal data via multidimensional P-Splines"
author: "Tayler Blake"
date: "June 19, 2017"
output: 
  tufte::tufte_html: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE)
library(plotly)
library(plyr)
library(dplyr)
library(lubridate)
library(scales)
library(stringr)
library(alabama)
library(nnet)
library(caret)
library(e1071)
library(splines)
library(magrittr)
library(ggplot2)
library(plyr)
library(dplyr)
library(tufte)
```
<br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<br><br><br><br><br><br>


## An estimate of a covariance matrix is a necessary (evil) prerequisite of almost any statistical analysis that includes a model. 
`r newthought("But covariance estimation is hard!")`

<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>


Data: $Y_i = \left( Y_{i1}, Y_{i2}, \dots, Y_{im} \right)^\prime, \qquad i=1,\dots, N$ with measurement times

$$t_{i1} < t_{i2} < \dots< t_{i,m_i}$$
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>


`r newthought("Covariance matrices should be positive definite.")`

$\Rightarrow$ `r newthought("constrained optimization = potential computational nightmare.")`

<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>


`r newthought("The")` $\left\{t_{ij} \right\}$ `r newthought("may not be common for all subjects, nor be observed on a a regular grid.")`
    
$\Rightarrow$ `r newthought("many of the existing approaches fail to accomodate this.")`

<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>

`r newthought("The sample covariance matrix is unstable in high dimensions.")`

$\Rightarrow$ `r newthought("Need to regularize estimates in some reasonable way.")`


<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>


# Covariance matrices should be positive definite.
`r newthought("We can use a cute little reparameterization of the observed covariance matrix.")` 

## The modified Cholesky decomposition:

For now, let $Y = \left(Y_1, \dots, Y_M \right)^\prime \sim \mathcal{N}\left(0,\Sigma\right)$. For any positive definite matrix $\Sigma$, we can find a unique unit lower triangular matrix $T$

$$ T = \begin{bmatrix} 1 & 0 & \dots & & \\ -\phi_{21} & 1 & & & \\ -\phi_{31}& -\phi_{32} &  1 & & \\ \vdots & & & \ddots & \\ -\phi_{M1} &-\phi_{M2} & \dots & -\phi_{M,M-1}& 1  \end{bmatrix} $$

which diagonalizes $\Sigma$:

\begin{equation}
\nonumber T \Sigma T^\prime = D
\end{equation}
`r margin_note("")`

<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>

## I know, the cuteness isn't obvious yet. But think about this:


`r newthought("many of the existing approaches fail to accomodate this.")`
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br> <br><br><br><br><br><br>
```{r simple-surface}
simple_surface <- expand.grid(x1 = 1:10, x2 = 1:10)
simple_surface$y <- 2 + .5 * simple_surface$x1 - 1 * simple_surface$x2

plot_ly(x = 1:10, y = 1:10, z = matrix(simple_surface$y, 10, 10, byrow = TRUE)) %>% 
  add_surface() %>%
  layout(scene = list(xaxis = list(title = "x1"),
                      yaxis = list(title = "x2"),
                      zaxis = list(title = "y",
                                   range = c(-10, 10))))

```
-->

<!--
```{r simple-regression-example}
set.seed(1977)
reg_frame <- data.frame(x = runif(100))
reg_frame$y <- -3 + 6 * reg_frame$x + rnorm(100, 0, .5)
reg_frame$fit <- predict(lm(y ~ x, data = reg_frame))

plot_ly(reg_frame, x = ~x, y = ~y, type = "scatter", mode = "markers",
        name = "observed values",
        marker = list(size = 10,
                       color = 'rgba(255, 182, 193, .9)',
                       line = list(color = 'rgba(152, 0, 0, .8)',
                                   width = 2)))

```
-->
